% vim: syntax=tex:
\section{Related Works}

Current approaches to accelerating research computations and engineering
applications include parallel computing and distributed computing. A parallel
system is when several processing elements work simultaneously to solve a
problem faster. The primary consideration is elapsed time, not throughput or
sharing of resources at remote locations. A distributed system is a collection
of independent computers that appears to its users as a single coherent system.
In distributed computing the data is split into smaller parts and subsequently
spread across the system. The result is then collected and outputted. Summary
of current solutions are listed in \cref{tab:relatedProjects} below followed by
detailed description of every entry.\\

\begin{table}[htb]
\centering
\begin{tabular}{ll}
\toprule{}
\textbf{Project}   & \textbf{Description} \\
\midrule{}
\texttt{Hadoop}    & Distributed computation framework
                     \cite{website:Hadoop-Wiki} \cite{website:Apache-Hadoop}
                     \cite{shvachko2011apache}\\
\midrule{}
\texttt{BOINC}     & Volunteer and grid computing project distributed
                     \cite{anderson2004boinc} \\
\midrule{}
\texttt{GPUDirect} & \gls{gpu}-to-\gls{gpu} framework for parallel computing
                     \cite{website:YouTube} \\
\midrule{}
\texttt{MPI}       & Message-passing system for parallel computations
                     \cite{website:MPI-Tutorial} \cite{website:mpi-4-python} \\
\midrule{}
\texttt{PVM}       & Distributed environment and message passing system
                     \cite{website:Computer-Science-and-Division} \\
\bottomrule{}
\end{tabular}
\caption{Summary of Current Approaches and Projects}
\label{tab:relatedProjects}
\end{table}

\subsection{Hadoop}

Hadoop provides a distributed file system and a framework
for the analysis and transformation of very large data sets using the MapReduce
paradigm \cite{website:Apache-Hadoop} \cite{website:Apress} \cite{dias2011hpc}
\cite{dean2001mapreduce}. An important characteristic of Hadoop is the
partitioning of data and computation across many (thousands) of hosts (nodes),
and executing application computations in parallel close to their data
\cite{shvachko2011apache}. The MapReduce \cite{luo2011hierarchical}
\cite{website:Hadoop-Wiki-map} paradigm which Hadoop implements is
characterized by dividing the application into many small fragments of work,
each of which may be executed or re-executed on any node in the cluster. The
worker node processes the smaller problem, and passes the answer back to its
master node. The answers to the subproblems are then combined to form the
output. Hadoop is popular model for distributed computations, however, it is
does not integrate with \gls{cuda}-enabled \glspl{gpu}.

\subsection{BOINC}

The \Gls{boinc} is an open source middleware system for volunteer and grid
computing. The general public can contribute to today's computing power by
donating their personal computer's disk space and some percentage of \gls{cpu}
and \gls{gpu} processing. In other words, \gls{boinc} is software that can use
the unused \gls{cpu} and \gls{gpu} cycles on a computer to do scientific
computing \cite{anderson2004boinc}. The \gls{boinc} project allows for
distributed computing using hundreds of millions of personal computers and game
consoles belonging to the general public. This paradigm enables previously
in-feasible research and scientific super-computing. The framework requires
that individuals are connected to the Internet and is supported by various
operating systems, including Microsoft Windows, Mac OS X and various Unix-like
systems. Although \gls{boinc} is another step in grid computing, it is does
not seem to be relevant to our research.

\subsection{GPUDirect}

Currently \gls{gpu} based clusters are becoming more popular.
\texttt{GPUDirect} is a \gls{gpu} based clustering using Infiniband, an
architecture specification that defines a connection between processor nodes.
\texttt{GPUDirect} allows for multiple \glspl{gpu} to communicate with each
other directly by giving \glspl{gpu} ability to initiate and manage memory
transfer \cite{website:YouTube}. Prior to GPUDirect, \gls{gpu}-to-\gls{gpu}
communication involved the \gls{cpu}. \texttt{GPUDirect} performance gain
exceed \gls{cpu} solutions. Direct \gls{gpu}-to-\gls{gpu} communication is less
computationally expensive since less messages will be sent and received via the
host server. An issue with \texttt{GPUDirect} is that it requires specific
hardware. The proposed framework will serve as software solution to avoid
hardware limitations.


\subsection{MPI}

\Gls{mpi} is the dominant message passing programming paradigm for clusters
\cite{website:Message-Passing-Interface-Forum}
\cite{website:Message-Passing-Interface}. \gls{mpi} is a standardized and
portable message-passing system that functions on a wide variety of parallel
computers. Although the standard \gls{mpi} defines the syntax and semantics of
a core of library routines in the \texttt{C} programming language, it is a
language-independent communications protocol used to program parallel
computers. \texttt{CUDA+MPI} builds upon this model due to the fact that
\gls{mpi} is the dominant model used in high-performance computing
\cite{sur2006high}. \Gls{mpi} has been implemented for almost every distributed
memory architecture and is optimized for the hardware on which it runs.

\subsection{PVM}

\Gls{pvm} is a software tool for parallel networking of computers. It is
designed to allow a network of heterogeneous Unix and/or Windows machines to be
used as a single distributed parallel processor. Thus, large computational
problems can be solved more cost effectively by using the aggregate power and
memory of many computers. \gls{pvm} enables users to exploit their existing
computer hardware to solve much larger problems at less additional cost
\cite{website:Computer-Science-and-Division}. \gls{pvm} was a step towards
modern trends in distributed processing and grid computing but has, since the
mid-1990s, largely been supplanted by the much more successful \gls{mpi}
standard for message passing on parallel machines.

\subsection{CUDA+MPI}

The proposed \texttt{CUDA+MPI} framework will take advantage of both \gls{mpi}
and \gls{cuda} to perform computations on \gls{gpu} cards of computers
participating in the cluster. \texttt{CUDA+MPI}'s goal is to allow a collection
of heterogeneous computers each containing a \gls{cuda}-capable \gls{gpu} to be
used as a coherent and flexible concurrent computational resource.
