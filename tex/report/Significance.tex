% vim: syntax=tex:
\section{Significance}

To gain a better understanding of the significance of the problem let's examine
some pseudo code for computing element-wise vector summation.

\subsection{\texttt{CUDA} and \texttt{MPI} Vector Summation}

First, let's look at the pseudo code of the vector summation without the
framework. Notice, this code is executed on each node, $rank$ is the current
node we are using. This simplifies  on the \gls{mpi} side; but we can just as
easily do this differently.

\begin{algorithm}
\begin{algorithmic}
\Function{add}{$slice\_a,slice\_b$}
    \Comment{Actual addition is done on GPU}
    \State{$slice\_c \gets slice\_a + slice\_b$}
    \State{\Return{$slice\_c$}}
\EndFunction{}
\Function{Compute}{$card\_max,world\_size,N,a,b$}
    \Comment{Split Data and Compute Sum}
    \Comment{Ran on all nodes part of the \texttt{mpi} world}
    \State{$c \gets empty\_like(a)$}
    \State{$M \gets floor((N + card\_max - 1) / card\_max)$}
    \State{$m \gets floor((M + world\_size - 1) / world\_size)$}
    \For{$i < m$}
        \State{$slice\_low \gets (rank * 2 + i) * card\_max$}
        \State{$slice\_high \gets (rank * 2 + (i + 1)) * card\_max$}
        \State{$c[slice\_low:slice\_high] \gets add(a[slice\_low:slice\_high],
                                                   b[slice\_low:slice\_high])$}
    \EndFor{}
    \State{\Return{$c$}}
\EndFunction{}
\end{algorithmic}
\caption{Algorithm of \texttt{CUDA} and \texttt{MPI} Element-wise Vector
Summation}
\label{alg:cuda_mpi_vsum}
\end{algorithm}

The \texttt{ADD} function is uninteresting but defined for completeness. The
\texttt{COMPUTE} function, on the other hand, has some complexities and other
difficulties. Namely, the setting of $M$ and $m$, the integer number of groups
to be computed and the integer number of slices per node to be computed,
respectively. You can notice our problem code (element-wise vector summation)
is tightly coupled with the slicing/ division code. Further, setting $M$ and
$m$ can be complicated and difficult to get correct, distracting the
researcher/ developer from the real problem. This is a simple problem too.

\subsection{\texttt{OpenCUDA+MPI} Vector Summation}

The user code will be run on all minion nodes of the \gls{mpi} job. Further,
because the user code doesn't have to deal with splitting the data or sending
the data to other nodes, it is incredibly simple.

\begin{algorithm}
\begin{algorithmic}
\Function{vector\_add}{$data$}
    \Comment{Element-Wise Vector Summation}
    \State{$slice\_c \gets data[0] + data[1]$}
    \State{\Return{$slice\_c$}}
\EndFunction{}
\end{algorithmic}
\caption{User code of the framework}
\label{alg:ocmf_user_code}
\end{algorithm}

The framework algorithm can be found in the appendix \cref{alg:ocmf}.

Once the user code is put together with the framework using \cref{alg:run_code}
we will be given the same result as the first example (that doesn't use the
framework). The benefit of the separation is that the user function is easier
to manage and isn't coupled to other confounding code. Both of these niceties
should grant the benefit of better and easier to understand distributed and
parallel code.
